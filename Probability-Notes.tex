\documentclass[device=normal, lang=en, fontsize=12pt]{elegantnote}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{float}
\usepackage{extarrows}

\definecolor{pgcolor}{RGB}{251, 250, 248}
\pagecolor{pgcolor}
\numberwithin{equation}{section}

\theoremstyle{definition} %
\newtheorem{property}{Property}[section] %

\title{Probability Notes}
\author{FHYQ-Dong}
\date{\today}


\begin{document}

\maketitle
\newpage
\tableofcontents
\newpage


\input{secs/sec_Probability_Space.tex}
\input{secs/sec_Conditional_Probability.tex}
\input{secs/sec_Independence.tex}

\section{Discrete Random Variables}
\subsection{Random Variables}
Random variables (RV) is not a variable, but a diterministic function that maps the sample space to the real number, whose range is either finite or countably infinite. Notation: $X(\omega) = x$.
\begin{definition}[Random Variable]
    Given a probability space $(\varOmega, \mathcal{F}, \mathbf{P})$, a random variable $X$ is a function $X: \varOmega \rightarrow \mathbb{R}$ such that for any $x \in \mathbb{R}$, the set $\{\omega \in \varOmega: X(\omega) \leq x\} \in \mathcal{F}$.
\end{definition}
The intuition of introducing RV is to map the sample space to $\mathbb{R}$ so that we can apply some mathmatical tools to analyze the probability distribution of the sample space. But the probability meature on $\mathbb{R}$ is defined on Borel field $\mathcal{B}(\mathbb{R})$, \textbf{so if we want to calculate the probability of RV}, we should consider the a set $A \in \mathcal{B}(\mathbb{R})$. To maintain the consistency between RV and events, the pre-image of $A$ under $X$ should be in $\mathcal{F}$, i.e. for any $x \in \mathbb{R}$, the set $\{\omega \in \varOmega: X(\omega) \leq x\} \in \mathcal{F}$.
\begin{remark}
    The requirement $\forall x \in \mathbb{R},~ \{\omega \in \varOmega: X(\omega) \leq x\} \in \mathcal{F}$ is naturally met when the events in $\varOmega$ are discrete, because we can choose $\mathcal{F} = 2^{\varOmega}$.
\end{remark}
\begin{example}[Examples of RV] ~ \\ 
    A sequence of 5 tosses of a coin:
    \begin{itemize}
        \item The number of heads in the sequence [Discrete RV]
        \item The 5-long sequence is not a RV.
    \end{itemize}
    Two rolls of a die:
    \begin{itemize}
        \item The sum of two rolls [Discrete RV]
        \item The second roll to the power of the first roll [Discrete RV]
    \end{itemize}
    DeepSeek's inference:
    \begin{itemize}
        \item The latency in generating a response [Continuous RV]
        \item The number of requests in a second [Discrete RV]
    \end{itemize}
\end{example}

\subsection{Probability Mass Function}
\begin{definition}[Probability Mass Function]
    Probability mass function (PMF) is the ``probability law'' or ``probability distribution'' of a \textbf{discrete RV} $X$.
    \begin{align}
        p_{X}(x) &= \mathbf{P}(X = x) = \\ 
                 &= \mathbf{P}(\{\omega \in \varOmega: X(\omega) = x\})
    \end{align}
\end{definition}
Several properties of PMF can be derived from the axioms of probability, e.g. $p_{X}(x) \geq 0$, $\sum_{x} p_{X}(x) = 1$.
\begin{example}[Binomial PMF]
    \begin{align}
        p_{X}(k) &= \binom{n}{k} p^{k} (1-p)^{n-k}
    \end{align}
    One example: the number of heads in $n$ independent tosses of a coin.
\end{example}
\begin{example}[Geometric PMF]
    \begin{align}
        p_{X}(k) &= (1-p)^{k-1} p
    \end{align}
    One example: the number of tosses until the first head appears.
\end{example}
\begin{example}[Poisson PMF]
    \begin{align}
        p_{X}(k) = \mathrm{e}^{-\lambda} \frac{\lambda^{k}}{k!}
    \end{align}
    It's a good approximation of the binomial distribution when $n$ is large and $p$ is small.
    \begin{proof}
        Let $p = \frac{\lambda}{n}$, then
        \begin{align}
            \binom{n}{k} p^{k} (1-p)^{n-k} &= \frac{n!}{k!(n-k)!} p^{k} (1-p)^{n-k} \\
            &= \frac{n(n-1)\cdots(n-k+1)}{k!} p^{k} (1-p)^{n-k} \\
            &\approx \frac{n^{k}}{k!} p^{k} (1-p)^{n-k} \\
            &= \frac{n^{k}}{k!} \left(\frac{\lambda}{n}\right)^{k} \left(1-\frac{\lambda}{n}\right)^{n} \\
            &\approx \frac{n^{k}}{k!} \left(\frac{\lambda}{n}\right)^{k} \mathrm{e}^{-\lambda} \\
            &= \mathrm{e}^{-\lambda} \frac{\lambda^{k}}{k!}
        \end{align}
    \end{proof}
\end{example}
Note that all these PMFs are normalized.

\subsection{Functions of Random Variables}
\begin{definition}[Function of Random Variables]
    Given a RV $X$, a function $Y = g(X)$ is also a RV. If $X$ is discrete, then $Y$ is also discrete, with PMF $p_{Y}$ given by
    \begin{align}
        p_{Y}(y) = \sum_{x: g(x) = y} p_{X}(x)
    \end{align}
\end{definition}
\begin{remark}
    The function $g$ is defined on $\mathbb{R}$, i.e. $g: \mathbb{R} \rightarrow \mathbb{R}$. The meaning of $Y = g(X)$ in fact is $Y(\omega) = g(X(\omega))$.
\end{remark}

\subsection{Expectation and Variance}
\begin{definition}[Expectation of Random Variables]
    The expected value (also called the expectation or the mean) of a random variable $X$, with PMF $p_{X}$, is defined as
    \begin{align}
        \mathbf{E}[X] = \sum_{x} x p_{X}(x)
    \end{align}
\end{definition}
While the PMF gives us the full information about the distribution of $X$, sometimes we still need an overall measure of $X$.
\begin{property}[Properties of Expectation] ~ 
    \begin{itemize}
        \item Let $X$ be a RV and $Y = g(X)$. Sometimes $\mathbf{E}[Y] = \sum_{y} y p_{Y}(y)$ is hard to calculate. Instead, we can use $\mathbf{E}[Y] = \sum_{x} g(x) p_{X}(x)$ which is easier.
        \begin{proof}
            \begin{align}
                \mathbf{E}[Y] &= \sum_{y} y p_{Y}(y) \\
                              &= \sum_{y} y \sum_{x: g(x) = y} p_{X}(x) \\
                              &= \sum_{x} g(x) p_{X}(x)
            \end{align}
        \end{proof}
        \item In general, $\mathbf{E}[g(X)] \neq g(\mathbf{E}[X])$.
        \item If $\alpha_1, \alpha_2, \beta$ are constants, $X, Y$ are RVs, then
        \begin{align}
            \mathbf{E}[\alpha_1 X + \alpha_2 Y + \beta] = \alpha_1 \mathbf{E}[X] + \alpha_2 \mathbf{E}[Y] + \beta
        \end{align}
    \end{itemize}
\end{property}
\begin{definition}[Variance of Random Variables]
    The variance of a RV $X$ is defined as
    \begin{align}
        \mathrm{var}(X) = \mathbf{E}[(X - \mathbf{E}[X])^{2}]
    \end{align}
\end{definition}
Remember $\mathbf{E}[g(x)] = \sum_{x} g(x) p_{X}(x)$, then $\mathrm{var}(X) = \sum_{x} (x - \mathbf{E}[X])^{2} p_{X}(x)$.
We can also define the standard deviation as $\sigma(X) = \sqrt{\mathrm{var}(X)}$.
\begin{property}[Properties of Variance] ~ 
    \begin{itemize}
        \item Define second moment as $\mathbf{E}[X^2] = \sum_x x^2 p_{X}(x)$, then $\mathrm{var}(X)$ can be written in terms of moments expression $\mathrm{var}(X) = \mathbf{E}[X^{2}] - \mathbf{E}[X]^{2}$.
        \item $\mathrm{var}(X) \geq 0$.
        \item $\mathrm{var}(\alpha X + \beta) = \alpha^{2} \mathrm{var}(X)$.
    \end{itemize}
\end{property}
\begin{example}[Expectation and Variance of Binomial Distribution]
    \begin{align}
    &\begin{aligned}
        \mathbf{E}[X] &= \sum_{k=0}^{n} k \binom{n}{k} p^{k} (1-p)^{n-k} \\
                      &= \sum_{k=1}^{n} np \binom{n-1}{k-1} p^{k-1} (1-p)^{n-k} \\
                      &= np \sum_{k=1}^{n} \binom{n-1}{k-1} p^{k-1} (1-p)^{n-k} \\
                      &= np \sum_{k=0}^{n-1} \binom{n-1}{k} p^{k} (1-p)^{n-1-k} \\
                      &= np \cdot \mathrm{PMF} = np
    \end{aligned} \\
    &\begin{aligned}
        \mathbf{E}[X^{2}] =& \sum_{k=0}^{n} k^{2} \binom{n}{k} p^{k} (1-p)^{n-k} \\
                         =& np \sum_{k=1}^{n} k \binom{n-1}{k-1} p^{k-1} (1-p)^{n-k} \\
                         =& np \sum_{k=1}^{n} (k-1+1) \binom{n-1}{k-1} p^{k-1} (1-p)^{n-k} \\ 
                         =& n(n-1)p^2 \sum_{k=2}^{n} \binom{n-2}{k-2} p^{k-2} (1-p)^{n-k} \\ 
                         &+ np \sum_{k=1}^{n} (k-1+1) \binom{n-1}{k-1} p^{k-1} (1-p)^{n-k} \\ 
                         =& n(n-1)p^2 + np
    \end{aligned} \\
    &\begin{aligned}
        \mathrm{var}(X) &= \mathbf{E}[X^{2}] - \mathbf{E}[X]^{2} \\
                        &= n(n-1)p^2 + np - n^2p^2 \\
                        &= np(1-p)
    \end{aligned}
    \end{align}
    Let $n = 1$, then the binomial distribution is the Bernoulli distribution.
\end{example}
\begin{example}[Expectation and Variance of Poisson Distribution]
    \begin{align}
    &\begin{aligned}
        \mathbf{E}[X] &= \sum_{k=0}^{\infty} k \mathrm{e}^{-\lambda} \frac{\lambda^{k}}{k!} \\
                      &= \lambda \mathrm{e}^{-\lambda} \sum_{k=1}^{\infty} \frac{\lambda^{k-1}}{(k-1)!} \\
                      &= \lambda \mathrm{e}^{-\lambda} \mathrm{e}^{\lambda} \\
                      &= \lambda
    \end{aligned} \\
    &\begin{aligned}
        \mathbf{E}[X^{2}] &= \sum_{k=0}^{\infty} k^{2} \mathrm{e}^{-\lambda} \frac{\lambda^{k}}{k!} \\
                         &= \lambda \mathrm{e}^{-\lambda} \sum_{k=1}^{\infty} k \frac{\lambda^{k-1}}{(k-1)!} \\
                         &= \lambda \mathrm{e}^{-\lambda} \sum_{k=1}^{\infty} (k-1+1) \frac{\lambda^{k-1}}{(k-1)!} \\
                         &= \lambda \mathrm{e}^{-\lambda} \left(\lambda \sum_{k=1}^{\infty} \frac{\lambda^{k-1}}{(k-1)!} + \sum_{k=1}^{\infty} \frac{\lambda^{k-1}}{(k-1)!}\right) \\
                         &= \lambda \mathrm{e}^{-\lambda} (\lambda \mathrm{e}^{\lambda} + \mathrm{e}^{\lambda}) \\
                         &= \lambda^{2} + \lambda
    \end{aligned} \\
    &\begin{aligned}
        \mathrm{var}(X) &= \mathbf{E}[X^{2}] - \mathbf{E}[X]^{2} \\
                        &= \lambda^{2} + \lambda - \lambda^{2} \\
                        &= \lambda
    \end{aligned}
    \end{align}
\end{example}

\end{document}
