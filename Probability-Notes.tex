\documentclass[device=normal, lang=en]{elegantbook}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{float}
\usepackage{extarrows}
\usepackage{physics}

\input{math_cmds.tex}

\definecolor{pgcolor}{RGB}{251, 250, 248}
\pagecolor{pgcolor}
\numberwithin{equation}{section}

% \theoremstyle{definition} %
% \newtheorem{property}{Property}[section] %

\title{Probability Notes}
\author{FHYQ-Dong}
\date{\today}
\version{8.0}
\cover{images/Note-cover.png}
\definecolor{customcoverlinecolor}{RGB}{82, 59, 148}
\colorlet{coverlinecolor}{customcoverlinecolor}


\begin{document}

\maketitle
\frontmatter

\tableofcontents
\mainmatter


\input{chaps/chap_Probability_Space.tex}
\input{chaps/chap_Conditional_Probability.tex}
\input{chaps/chap_Independence.tex}
\input{chaps/chap_Discrete_Random_Variables.tex}
\input{chaps/chap_Multiple_Random_Variables.tex}
\input{chaps/chap_Continuous_Random_Variables.tex}
\input{chaps/chap_Multiple_Continuous_Random_Variables.tex}




\chapter{Derived Distributions and Entropy}

\section{Derived Distributions}
\label{sec:derived-distributions}
We've already known the Distribution of a RV $X$ and we want to find the distribution of $Y = g(X)$. In discrete case we can calculate the PMF for each possible value of $Y$ like $\mathbf{P}(Y = y) = \sum_{x: g(x) = y} \mathbf{P}(X = x)$. For continuous case we cannot simply do such thing because the conservation of PDF is not guaranteed under summation. Instead, CDF is standardized and we can use it to find the PDF of $Y$.
\begin{theorem}[Principal Method for Derived Distribution]
    Two-step method to find the distribution of $Y = g(X)$:
    \begin{enumerate}
        \item Find the CDF of $Y$:
        \begin{equation}
            F_{Y}(y) = \mathbf{P}(Y \leq y) = \mathbf{P}(g(X) \leq y)
        \end{equation}
        \item Differentiate the CDF to find the PDF:
        \begin{equation}
            f_{Y}(y) = \dv{F_{Y}}{y}
        \end{equation}
    \end{enumerate}
\end{theorem}

\begin{example}[Taking a Train]
    A professor is taking the high-speed railway from Beijing to Shanghai. Suppose that the speed of the train is uniformly distributed between 280 km/h and 350 km/h. The distance between the two cities is 1400 km. What is the distribution of the duration of the trip?
\end{example}
\begin{solution}
    Let $T(V) = \frac{1400}{V}$ be the duration of the trip. The speed $V$ is uniformly distributed in $[280, 350]$, so the CDF of $V$ is
    \begin{equation}
        F_{V}(v) = \begin{cases}
            0, & v < 280 \\ 
            \frac{v - 280}{350 - 280}, & 280 \leq v < 350 \\ 
            1, & v \geq 350
        \end{cases}
    \end{equation}
    The CDF of $T$ is
    \begin{equation}
        F_{T}(t) = \mathbf{P}(T \leq t) = \mathbf{P}\left(\frac{1400}{V} \leq t\right) = \mathbf{P}\left(V \geq \frac{1400}{t}\right) = 1 - F_{V}\left(\frac{1400}{t}\right)
    \end{equation}
    \begin{equation}
        F_{T}(t) = \begin{cases}
            0, & t < 4 \\ 
            5 - \frac{20}{t}, & 4 \leq t < 5 \\ 
            1, & t \geq 5
        \end{cases}
    \end{equation}
    The PDF of $T$ is
    \begin{equation}
        f_{T}(t) = \dv{F_{T}}{t} = \begin{cases}
            0, & t < 4 \\ 
            \frac{20}{t^2}, & 4 \leq t < 5 \\ 
            0, & t \geq 5
        \end{cases}
    \end{equation}
\end{solution}

\begin{example}[PDF of $Y = aX+b$]
    Let $X$ be a RV with PDF $f_{X}(x)$, and $Y = aX + b$ where $a \neq 0$, then the PDF of $Y$ is
    \begin{equation}
        f_{Y}(y) = \frac{1}{\abs{a}} f_{X}\left(\frac{y - b}{a}\right)
    \end{equation}
\end{example}
\begin{solution}
    The CDF of $Y$ is
    \begin{equation}
        F_{Y}(y) = \mathbf{P}(Y \leq y) = \mathbf{P}(aX + b \leq y) = \mathbf{P}\left(X \leq \frac{y - b}{a}\right) = F_{X}\left(\frac{y - b}{a}\right)
    \end{equation}
    And then differentiating the CDF using chain rule
    \begin{equation}
        f_{Y}(y) = \dv{F_{Y}}{y} = \frac{1}{a} \dv{F_{X}}{\frac{y - b}{a}} = \frac{1}{a} f_{X}\left(\frac{y - b}{a}\right)
    \end{equation}
    If $a < 0$, the inequality will be reversed, and a minus sign will be added to the derivative. So we have
    \begin{equation}
        f_{Y}(y) = \frac{1}{\abs{a}} f_{X}\left(\frac{y - b}{a}\right)
    \end{equation}
\end{solution}

\begin{example}[CDF and PDF of Normal Distribution]
    The PDF of standard normal distribution is
    \begin{equation}
        f_{X}(x) = \frac{1}{\sqrt{2\pi}} \e^{-\frac{x^2}{2}}
    \end{equation}
    whose CDF cannot be expressed in closed form and is defined as
    \begin{equation}
        \Phi(x) = \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{x} \e^{-\frac{t^2}{2}} \dd{t}
    \end{equation}
    Now, a RV $Y$ is normally distributed with parameters $\mu$ and $\sigma^2$. Let $Z = \frac{Y - \mu}{\sigma}$, then $Z$ is standard normal distributed. The CDF of $Y$ is
    \begin{equation}
        F_{Y}(y) = \mathbf{P}(Y \leq y) = \mathbf{P}\left(\frac{Y - \mu}{\sigma} \leq \frac{y - \mu}{\sigma}\right) = \mathbf{P}(Z \leq \frac{y - \mu}{\sigma}) = \Phi\left(\frac{y - \mu}{\sigma}\right)
    \end{equation}
    And the PDF of $Y$ is
    \begin{equation}
        f_{Y}(y) = \dv{F_{Y}}{y} = \dv{\Phi\left(\frac{y - \mu}{\sigma}\right)}{y} = \frac{1}{\sigma} \Phi'\left(\frac{y - \mu}{\sigma}\right) = \frac{1}{\sigma} f_{Z}\left(\frac{y - \mu}{\sigma}\right) = \frac{1}{\sqrt{2\pi}\sigma} \e^{-\frac{(y - \mu)^2}{2\sigma^2}}
    \end{equation}
\end{example}

For monotonic functions $g(x)$
\begin{theorem}[PDF of a Strictly Monotonic Function]
    Suppose that $g(\cdot)$ is strictly monotonic, then $g$ has an inverse $h(\cdot) \triangleq g^{-1}(\cdot)$. Assume $h$ is differentiable and the PDF of $Y$ is
    \begin{equation}
        f_{Y}(y) = f_{X}(h(y)) \abs{\dv{h}{y}}
    \end{equation}
\end{theorem}
\begin{proof}
    \begin{align}
        F_{Y}(y) &= \mathbf{P}(Y \leq y) = \mathbf{P}(g(X) \leq y) = \mathbf{P}(X \leq h(y)) = F_{X}(h(y)) \\ 
        f_{Y}(y) &= \dv{F_{Y}}{y} = f_{X}(h(y)) \dv{h}{y}
    \end{align}
    If $g$ is strictly decreasing, the inequality will be reversed, and a minus sign will be added to the derivative. So we have
    \begin{equation}
        f_{Y}(y) = f_{X}(h(y)) \abs{\dv{h}{y}}
    \end{equation}
\end{proof}
\begin{remark}
    If $g$ is a multiple-to-multiple function, $\abs{\dv{h}{y}}$ becomes Jacobian determinant.
\end{remark}

\section{Entropy}
\begin{definition}[Entropy]
    The entropy of a discrete RV $X$ with PMF $p_X(x)$ is defined as
    \begin{equation}
        H(X) = -\sum_{x} p_X(x) \ln p_X(x)
    \end{equation}
    The entropy of a continuous RV $X$ with PDF $f_X(x)$ is defined as
    \begin{equation}
        H(X) = -\int_{-\infty}^{+\infty} f_X(x) \ln f_X(x) \dd{x}
    \end{equation}
\end{definition}
\begin{remark}
    \begin{itemize}
        \item The entropy of a discrete RV is non-negative, while the entropy of a continuous RV can be negative (if $f_{X}(x) > 1$).
        \item The entropy describes the uncertainty of a random experiment.
        \item The base of the logarithm can be different, which is equal to multiplying by a constant. \begin{itemize}
            \item If the base is 2, the unit is bit.
            \item If the base is $\e$, the unit is nat.
        \end{itemize}
    \end{itemize}
\end{remark}
\begin{remark} \\ 
    Why is entropy in the form of the expectation of $-\ln f_{X}(x)$? \\ 
    The entropy of a RV is defined as the expectation of the self-information $H(X) = \mathbf{E}[I(p(X))]$. According to Shannon, self-information shoudle satisfies the following axioms:
    \begin{itemize}
        \item Self-information is and only is a function of probability $I(p)$.
        \item If probability $p = p_1 p_2$, then $I(p) = I(p_1) + I(p_2)$.
        \item $I(p)$ is continuous.
    \end{itemize}
    The only function that satisfies these axioms is $I(p) = k \ln p$. We add a negative sign to make it positive, and the constant $k$ is equivalent to the base of the logarithm. 
\end{remark}

\begin{example}[Entropy of Bernoulli RV]
    Let $X$ be a Bernoulli RV with $\mathbf{P}(X = 1) = p$, $\mathbf{P}(X = 0) = 1 - p$. Find the entropy of $X$.
\end{example}
\begin{solution}
    THe entropy of $X$ in bits is
    \begin{equation}
        H(X) = -p \ln p - (1 - p) \ln (1 - p)
    \end{equation}
    which is maximized when $p = 0.5$ (the most uncertain case), and is minimized when $p = 0$ or $p = 1$ (the most certain case). 
\end{solution}


\appendix
\input{chaps/app_Problem_Solutions.tex}


\end{document}
