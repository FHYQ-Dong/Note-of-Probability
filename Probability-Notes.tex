\documentclass[device=normal, lang=en]{elegantbook}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{float}
\usepackage{extarrows}
\usepackage{physics}
\usepackage{mathtools}

\input{math_cmds.tex}

\definecolor{pgcolor}{RGB}{251, 250, 248}
\pagecolor{pgcolor}
\numberwithin{equation}{section}

% \theoremstyle{definition} %
% \newtheorem{property}{Property}[section] %

\title{Probability Notes}
\author{FHYQ-Dong}
\date{\today}
\version{11.0}
\cover{images/Note-cover.png}
\definecolor{customcoverlinecolor}{RGB}{82, 59, 148}
\colorlet{coverlinecolor}{customcoverlinecolor}


\begin{document}

\maketitle
\frontmatter

\tableofcontents
\mainmatter


\input{chaps/chap_Probability_Space.tex}
\input{chaps/chap_Conditional_Probability.tex}
\input{chaps/chap_Independence.tex}
\input{chaps/chap_Discrete_Random_Variables.tex}
\input{chaps/chap_Multiple_Random_Variables.tex}
\input{chaps/chap_Continuous_Random_Variables.tex}
\input{chaps/chap_Multiple_Continuous_Random_Variables.tex}
\input{chaps/chap_Derived_Distributions_and_Entropy.tex}
\input{chaps/chap_Convolution_Covariance_Correlation_and_Conditional_Expectation.tex}
\input{chaps/chap_Transforms_and_Sum_of_A_Random_Number_of_RVs.tex}


\chapter{Weak Law of Large Numbers}

\section{Probabilistic Inequalities}
The intuition makes sense that if a non-negative RV must have a little probability to be large if its mean is small.
\begin{theorem}[Markov's Inequality]
    If a RV $X$ is non-negative, then for any $a > 0$,
    \begin{equation}
        \mathbf{P}(X \geq a) \leq \frac{\E[X]}{a}
    \end{equation}
\end{theorem}
\begin{proof}
    \begin{equation}
        \E[X] = \int_{0}^{+\infty} xf_X(x) \dd{x} \geq \int_{a}^{+\infty} xf_X(x) \dd{x} \geq a\int_{a}^{+\infty} f_X(x) \dd{x} = a \mathbf{P}(X \geq a)
    \end{equation}
\end{proof}
The equality holds iff. $\int_{0}^{a} xf_X(x) \dd{x} = 0$ and $\int_{a^+}^{+\infty} xf_X(x) \dd{x} = 0$, i.e. $X$ can only take the value $a$ and $0$.

The intuition makes sense that if a RV has small variance, then it is unlikely to be far away from its mean.
\begin{theorem}[Chebyshev's Inequality]
    If a RV $X$ has finite mean $\mu$ and finite variance $\sigma^2$, then for any $c > 0$,
    \begin{equation}
        \mathbf{P}(\abs{X - \mu} \geq c) \leq \frac{\sigma^2}{c^2}
    \end{equation}
\end{theorem}
\begin{proof}
    \begin{equation}
        \sigma^2 = \int_{-\infty}^{+\infty} (x - \mu)^2 f_X(x) \dd{x} \geq \int_{-\infty}^{\mu - c} (x - \mu)^2 f_X(x) \dd{x} + \int_{\mu + c}^{+\infty} (x - \mu)^2 f_X(x) \dd{x} \geq c^2 \mathbf{P}(\abs{X - \mu} \geq c)
    \end{equation}
\end{proof}
The equality holds iff. $\int_{(\mu-c)^+}^{(\mu+c)^-} (x - \mu)^2 f_X(x) \dd{x} = \int_{-\infty}^{(\mu-c)^-} (x - \mu)^2 f_X(x) \dd{x} = \int_{(\mu+c)^{+}}^{+\infty} (x - \mu)^2 f_X(x) \dd{x} = 0$, i.e. $X$ can only take the value $\mu - c$, $\mu + c$ and $\mu$, and $\mathbf{P}(X = \mu) = 1 - \sigma^2/c^2$. To keep the mean is still $\mu$, $\mathbf{P}(X = \mu + c) = \mathbf{P}(X = \mu - c) = \sigma^2/c^2$.

\begin{theorem}[Chernoff's Inequality]
    If a RV $X$ has a transform $M_{X}(s)$, then
    \begin{equation}
        \mathbf{P}(X \geq a) \leq \exp\qty(-\max_{s \geq 0} \qty{sa - \ln M_{X}(s)})
    \end{equation}
\end{theorem}
\begin{proof}
    Given $a$ and $\textcolor{red}{\forall s \geq 0}$, we have
    \begin{equation}
        M_X(s) = \int_{-\infty}^{a} \e^{sx} f_X(x) \dd{x} + \int_{a}^{+\infty} \e^{sx} f_X(x) \dd{x} \geq \int_{a}^{+\infty} \e^{sx} f_X(x) \dd{x} \geq \e^{sa} \mathbf{P}(X \geq a)
    \end{equation}
    which yields
    \begin{equation}
        \mathbf{P}(X \geq a) \leq \frac{M_X(s)}{\e^{sa}}, \forall s \geq 0 \quad\Rightarrow\quad \mathbf{P}(X \geq a) \leq \inf_{s \geq 0} \frac{M_X(s)}{\e^{sa}} = \exp\qty(-\max_{s \geq 0} \qty{sa - \ln M_{X}(s)})
    \end{equation}
\end{proof}
This inequality is useful when $a > 0$. Similarly, we can prove
\begin{theorem}[Chernoff's Inequality]
    If a RV $X$ has a transform $M_{X}(s)$, then
    \begin{equation}
        \mathbf{P}(X \leq a) \leq \exp\qty(-\max_{s \leq 0} \qty{sa - \ln M_{X}(s)})
    \end{equation}
\end{theorem}

The three inequalities have the upper bound of $c^{-1}$, $c^{-2}$ and $\exp(-c)$, respectively, with the information of the distribution of $X$ increasing. 

\begin{example}[Upper Bound of Variance]
    When $X$ is known to take values in a range $[a, b]$, then $\sigma^2 \leq (b - a)^2/4$.
\end{example}
\begin{proof}
    Note that for any constant $\gamma$
    \begin{equation}
        \E[(X - \gamma)^2] = \E[X^2] - 2\gamma \E[X] + \gamma^2
    \end{equation}
    which is minimized when $\gamma = \E[X]$. Therefore, letting $\gamma = (a+b)/2$
    \begin{equation}
        \sigma^2 = \E[X - \E[X]] \leq \E\qty[\qty(X - \frac{a+b}{2})^2] = \E\qty[(X - a)(X - b)] + \frac{(b - a)^2}{4} \leq \frac{(b - a)^2}{4}
    \end{equation}
\end{proof}


\section{Weak Law of Large Numbers}
When the number of samples becomes large, i.e. let $X_1, X_2, \cdots, X_n$ be i.i.d RVs with finite mean $\mu$ and finite variance $\sigma^2$, and let $n \to \infty$
\begin{equation}
    M_n = \frac{X_1 + X_2 + \cdots + X_n}{n}
\end{equation}
\begin{itemize}
    \item $\E[M_n] = \mu$
    \item $\var(M_n) = \frac{\sigma^2}{n}$
    \item Applying Chebyshev's inequality
    \begin{equation}
        \mathbf{P}(\abs{X - \mu} \geq \epsilon) \leq \frac{\sigma^2}{n\epsilon^2}
    \end{equation}
    \item Let $n \to \infty$, we have $\mathbf{P}(\abs{X - \mu} \geq \epsilon) \to 0$.
\end{itemize}
\begin{theorem}[The Weak Law of Large Numbers]
    Let $X_1, X_2, \cdots, X_n$ be i.i.d RVs with finite mean $\mu$ and finite variance $\sigma^2$. Then for any $\epsilon > 0$,
    \begin{equation}
        \mathbf{P}(\abs{M_n - \mu} \geq \epsilon) = \mathbf{P}\qty(\abs{\frac{X_1 + X_2 + \cdots + X_n}{n} - \mu} \geq \epsilon) \leq \frac{\sigma^2}{n\epsilon^2} \to 0, \text{ as } n \to \infty
    \end{equation}
\end{theorem}
We say that $M_n$ converges \textcolor{red}{in probability} to $\mu$, denoted as $M_n \xrightarrow{\textcolor{red}{p}} \mu$.

\begin{example}[The pollster's problem]
    Let $f$ denote the fraction of voters who support a particular officer, and $X_i$ be Bernoulli random variables encoding poll responses
    \begin{equation}
        X_i = \left\{\begin{aligned}
            1, \quad &\text{if yes} \\
            0, \quad &\text{if no}
        \end{aligned}\right.
    \end{equation}
    The sample mean $M_n = (X_1 + \cdots + X_n) / n$ estimates the fraction of ``yes'' in our sample. The goal is to solve for $n$ satisfying 95\% confidence of $\leq 1\%$ error
    \begin{equation}
        \mathbf{P}(\abs{M_n - f} \geq 0.01) \leq 0.05
    \end{equation}
\end{example}
\begin{solution}
    Use the Chebyshev's inequality
    \begin{equation}
        \mathbf{P}(\abs{M_n - f} \geq 0.01) \leq \frac{\sigma_{M_n}^2}{(0.01)^2} = \frac{\sigma_X^2}{n(0.01)^2} \leq \frac{1}{4n(0.01)^2} \leq 0.05 \quad\Rightarrow\quad n \geq 50000
    \end{equation}
\end{solution}

\appendix
\input{chaps/app_Problem_Solutions.tex}


\end{document}
