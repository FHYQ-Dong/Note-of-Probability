\documentclass[device=normal, lang=en, fontsize=12pt]{elegantnote}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{float}
\usepackage{extarrows}

\definecolor{pgcolor}{RGB}{251, 250, 248}
\pagecolor{pgcolor}
\numberwithin{equation}{section}

\theoremstyle{definition} %
\newtheorem{property}{Property}[section] %

\title{Probability Notes}
\author{FHYQ-Dong}
\date{\today}


\begin{document}

\maketitle
\newpage
\tableofcontents
\newpage


\input{secs/sec_Probability_Space.tex}
\input{secs/sec_Conditional_Probability.tex}
\input{secs/sec_Independence.tex}
\input{secs/sec_Discrete_Random_Variables.tex}


\section{Multiple Random Variables}
\subsection{Conditioning}
\begin{definition}[Conditional PMF]
    The conditional PMF of $X$ given $A$ is defined as
    \begin{align}
        p_{X|A}(x) = \mathbf{P}(X = x | A) = \frac{\mathbf{P}(\{X = x\} \cap A)}{\mathbf{P}(A)}
    \end{align}
    Specifically, if given $Y = y$, then
    \begin{align}
        p_{X|Y = y}(x) = \mathbf{P}(X = x | Y = y) = \frac{\mathbf{P}(\{X = x\} \cap \{Y = y\})}{\mathbf{P}(Y = y)}
    \end{align}
\end{definition}
Since $\mathbf{P}(A) = \sum_{x} \mathbf{P}(\{X = x\} \cap A)$, we have $\sum_{x} p_{X|A}(x) = 1$.
The conditional PMF limits the sample space to $A$.
\begin{definition}[Conditional Expectation]
    The conditional expectation of $X$ given $A$ is defined as
    \begin{align}
        \mathbf{E}[X|A] = \sum_{x} x p_{X|A}(x)
    \end{align}
    Specifically, if given $Y = y$, then
    \begin{align}
        \mathbf{E}[X|Y = y] = \sum_{x} x p_{X|Y}(x|y)
    \end{align}
\end{definition}
\begin{theorem}[Total Expectation Theorem]
    Partition the sample space into disjoint events $A_{1}, A_{2}, \cdots$, then
    \begin{equation}
    \begin{aligned}
        \mathbf{P}(B) &= \sum_{i} \mathbf{P}(A_{i}) \mathbf{P}(B|A_{i}) \\ 
        p_{X}(x) &= \sum_{i} \mathbf{P}(A_{i}) p_{X|A_{i}}(x) \\
        \mathbf{E}[X] &= \sum_{i} \mathbf{P}(A_{i}) \mathbf{E}[X|A_{i}]
    \end{aligned}
    \end{equation}
    From line 2 to line 3, we multiply $x$ on both sides and sum over $x$.
    \begin{figure}[H]
        \centering
        \includegraphics[width=0.3\textwidth]{images/Note-5.1.png}
        \caption{Total Expectation Theorem}
    \end{figure}
\end{theorem}
\begin{example}[Geometric PMF 1]
    Toss a fair coin independently until a head occurs. Let $X$ be the number of tosses. What is the PMF, expectation, and variance of $X$? \\ 
    \textbf{Solution 1:} 
    \begin{equation}
    \begin{aligned}
        \mathbf{E}[X] &= \sum_{k=1}^{\infty} k \mathbf{P}(X = k) = \sum_{k=1}^{\infty} k (1-p)^{k-1} p = \frac{1}{p} \\ 
        \text{var}(X) &= \mathbf{E}[X^{2}] - \mathbf{E}[X]^{2} = \frac{1-p}{p^{2}}
    \end{aligned}
    \end{equation}
\end{example}
\begin{property}[Memoryless Property]
    Given that $X > 2$, the random variable $X - 2$ has same geometric PMF with $X$ (not given that $X > 2$). 
    \begin{equation}
    \begin{aligned}
        p_{X|X > 2}(x) &= \frac{\mathbf{P}(\{X > 2\} \cap \{X = x\})}{\mathbf{P}(X > 2)} \\ 
        &= \frac{(1 - p)^{k-1}}{1 - p - p(1-p)} = (1-p)^{k-3}p
    \end{aligned}
    \end{equation}
    If we \textit{shift} $k$ by 2, then the PMF is the same. That is to say, the random variable $X - 2$ given $X > 2$ has the same geometric PMF with $X$, and thus $\mathbf{E}[(X - 2)|X > 2] = \mathbf{E}[X]$.
\end{property}
\begin{example}[Geometric PMF 2]
    Toss a fair coin independently until a head occurs. Let $X$ be the number of tosses. What is the PMF, expectation, and variance of $X$? \\ 
    \textbf{Solution 2:} If we use the memoryless property, then 
    \begin{equation}
    \begin{aligned}
        \mathbf{E}[X] &= \mathbf{P}(X = 1)\mathbf{E}[X | X = 1] + \mathbf{P}(X > 1)\mathbf{E}[X | X > 1] \\ 
        &= p + (1-p)\mathbf{E}[(X - 1 + 1) | X > 1] \\
        &= p + (1-p)(1 + \mathbf{E}[X]) \\ 
        &\Rightarrow \mathbf{E}[X] = \frac{1}{p}
    \end{aligned}
    \end{equation}
    Similarly
    \begin{equation}
    \begin{aligned}
        \mathbf{E}[X^2] &= \mathbf{P}(X = 1)\mathbf{E}[X^2 | X = 1] + \mathbf{P}(X > 1)\mathbf{E}[X^2 | X > 1] \\
        &= p + (1-p)\mathbf{E}[(X - 1 + 1)^2 | X > 1] \\
        &= p + (1-p)(1 + 2\mathbf{E}[X] + \mathbf{E}[X^2]) \\
        &\Rightarrow \mathbf{E}[X^2] = \frac{2-p}{p^2}
    \end{aligned}
    \end{equation}
    Then
    \begin{equation}
        \text{var}(X) = \mathbf{E}[X^2] - \mathbf{E}[X]^2 = \frac{1-p}{p^2}
    \end{equation}
\end{example}

\subsection{Multiple Discrete Random Variables}
\begin{definition}[Joint PMF]
    The joint PMF of $X$ and $Y$ is defined as
    \begin{align}
        p_{X, Y}(x, y) = \mathbf{P}(\{X = x\} \cap \{Y = y\})
    \end{align}
\end{definition}
\begin{property}[Properties of Joint PMF] ~
    \begin{itemize}
        \item Added up to 1: $\sum_x \sum_y p_{X, Y}(x, y) = 1$.
        \item Marginal PMF: $p_{X}(x) = \sum_y p_{X, Y}(x, y)$, $p_{Y}(y) = \sum_x p_{X, Y}(x, y)$. (Total Probability Theorem)
        \item Conditional PMF: $p_{X|Y}(x|y) = \frac{p_{X, Y}(x, y)}{p_{Y}(y)}$, $p_{Y|X}(y|x) = \frac{p_{X, Y}(x, y)}{p_{X}(x)}$.
    \end{itemize}
\end{property}
If we have a RV function of multiple RVs: $Z = g(X, Y)$, then the PMF of $Z$ is
\begin{align}
    p_{Z}(z) = \sum_{x, y: g(x, y) = z} p_{X, Y}(x, y)
\end{align}
\begin{definition}[Expectation of Multiple Random Variables]
    Recall that 
    \begin{align*}
        \mathbf{E}[g(X)] = \sum_{x} g(x) p_{X}(x)
    \end{align*}
    Then the expectation of $g(X, Y)$ is
    \begin{align}
        \mathbf{E}[g(X, Y)] = \sum_{x, y} g(x, y) p_{X, Y}(x, y)
    \end{align}
\end{definition}

\end{document}
