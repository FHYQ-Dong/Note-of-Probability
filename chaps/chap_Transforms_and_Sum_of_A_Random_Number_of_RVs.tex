\chapter{Transforms and Sum of A Random Number of RVs}

\section{Transforms}
If we want to calculate the PDF of $Z = X + Y$, we need to do the convolution of $f_{X}$ and $f_{Y}$. But if we have a look from the frequency domain, we just need to multiply the Fourier transforms of $f_{X}$ and $f_{Y}$, which is much easier.
\begin{definition}[Definition of the Transform]
    The transform associated with a RV $X$ is a function $M_{X}(s)$ of a scalar parameter $s$ defined as
    \begin{equation}
        M_{X}(s) = \E[\e^{sX}] = \left\{\begin{aligned}
            \sum_{x} p_{X}(x) \e^{sx}, \quad &\text{if } X \text{ is discrete} \\
            \int_{-\infty}^{+\infty} f_{X}(x) \e^{sx} \dd{x}, \quad &\text{if } X \text{ is continuous}
        \end{aligned}\right.
    \end{equation}
\end{definition}
\begin{remark}
    \begin{itemize}
        \item In SS, Laplace transform is defined using $\e^{-st}$, while here we use $\e^{sx}$.
        \item Given $\int_{-\infty}^{+\infty} f_{X}(x) \dd{x} = 1$, the transform is always well defined when $\Re(s) = 0$.
        \item The transform only exists when $\Re(s) < \sigma_0$, while in SS it is $\Re(s) > \sigma_0$.
        \item \textbf{When applying transformation on $X$, e.g. $X \mapsto aX + b$, it will affect $\e^{s\textcolor{red}{X}}$ but not $f_{\textcolor{red}{X}}(x)$, i.e. $\e^{sX} \mapsto \e^{s(aX + b)}$ but $f_{X}(x)$ remains unchanged, which is different from the Laplace transform in SS.}
    \end{itemize}
\end{remark}
\noindent Sanity checks:
    \begin{equation}
    \begin{aligned}
        M_{X}(0) &= \int_{-\infty}^{+\infty} f_{X}(x) \dd{x} = 1 \\ 
        \abs{M_{X}(s)} = \abs{\int_{-\infty}^{+\infty} f_{X}(x) \e^{sx} \dd{x}} &\textcolor{red}{~\leq} \int_{-\infty}^{+\infty} \abs{f_{X}(x)} \cdot \abs{\e^{sx}} \dd{x} = \int_{-\infty}^{+\infty} f_{X}(x) \dd{x} = 1
    \end{aligned}
    \end{equation}
Linear operations:
\begin{equation}
\begin{aligned}
    M_{aX}(s) &= \int_{-\infty}^{+\infty} f_{X}(x) \e^{s(ax)} \dd{x} = M_{X}(sa) \\
    M_{X + c}(s) &= \int_{-\infty}^{+\infty} f_{X}(x) \e^{s(x + c)} \dd{x} = M_{X}(s) \e^{sc} \\ 
    M_{aX + b} &= \int_{-\infty}^{+\infty} f_{X}(x) \e^{s(ax + b)} \dd{x} = M_{X}(sa) \e^{sb}
\end{aligned}
\end{equation}
From transforms to moments:
\begin{equation}
\begin{aligned}
    \dv{s} M_{X}(s) \Big|_{s=0} &= \int_{-\infty}^{+\infty} x \e^{sx} f_{X}(x) \dd{x} \Big|_{s=0} = \int_{-\infty}^{+\infty} x f_{X}(x) \dd{x} = \E[X] \\
    \dv[n]{s} M_{X}(s) \Big|_{s=0} &= \int_{-\infty}^{+\infty} x^n \e^{sx} f_{X}(x) \dd{x} \Big|_{s=0} = \int_{-\infty}^{+\infty} x^n f_{X}(x) \dd{x} = \E[X^n]
\end{aligned}
\end{equation}
\begin{equation}
    \mathbf{P}(X = c) = \lim_{N \to \infty} \frac{1}{N} \sum_{k=1}^{N} M_{X}(jk) \e^{-jk c} \quad \text{for interger-valued RV}
\end{equation}
\begin{proof}
    \begin{equation}
        \text{RHS} = \lim_{N \to \infty} \frac{1}{N} \sum_{k=1}^{N} \sum_{x} p_{X}(x) \e^{jkx} \e^{-jk c} = \lim_{N \to \infty} \frac{1}{N} \sum_{x} p_{X}(x) \sum_{k=1}^{N} \e^{jk(x - c)} 
    \end{equation}
    For $x = c$
    \begin{equation}
        \sum_{k=1}^{N} \e^{jk(x - c)} = \sum_{k=1}^{N} 1 = N
    \end{equation}
    For $x \neq c$
    \begin{equation}
        \sum_{k=1}^{N} \e^{jk(x - c)} = \frac{\e^{jk(x - c)}}{1 - e^{jk(x - c)}} \cdot \qty(1 - \e^{jkN(x - c)})
    \end{equation}
    which is bounded. Therefore, as $N \to \infty$
    \begin{equation}
        \text{RHS} = p_{X}(c)
    \end{equation}
\end{proof}

\begin{example}[Example of the Transform]
    \begin{itemize}
        \item Poisson RV: $X \sim \text{Poi}(\lambda)$
        \begin{equation}
            M_{X}(s) = \sum_{x=0}^{+\infty} \e^{sx} \frac{\lambda^x}{x!} \e^{-\lambda} = \e^{-\lambda} \sum_{x=0}^{+\infty} \frac{(\lambda \e^{s})^x}{x!} = \e^{-\lambda} \e^{\lambda \e^{s}} = \e^{\lambda(\e^{s} - 1)}
        \end{equation}
        \item Exponential RV: $X \sim \text{Exp}(\lambda)$
        \begin{equation}
            M_{X}(s) = \int_{0}^{+\infty} \lambda \e^{-\lambda x} \e^{sx} \dd{x} = \int_{0}^{+\infty} \lambda \e^{-(\lambda - s)x} \dd{x} = \frac{\lambda}{\lambda - s} \quad \Re(s) < \lambda
        \end{equation}
        \item Normal RV: $X \sim \mathcal{N}(\mu, \sigma^2)$ \\ 
        For standard normal RV $Z \sim \mathcal{N}(0, 1)$, we have
        \begin{equation}
            M_{Z}(s) = \int_{-\infty}^{+\infty} \frac{1}{\sqrt{2\pi}} \e^{-x^2/2} \e^{sx} \dd{x} = \e^{s^2/2} 
        \end{equation}
        Now $X = \sigma Z + \mu$, therefore
        \begin{equation}
            M_{X}(s) = M_{Z}(\sigma s) \e^{\mu s} = \e^{\sigma^2 s^2/2+ \mu s}
        \end{equation}
    \end{itemize}
\end{example}

\begin{example}[Moments of Exponential RV]
    For an exponential RV $X \sim \text{Exp}(\lambda)$, we have
    \begin{equation}
    \begin{aligned}
        M_{X}(s) &= \frac{\lambda}{\lambda - s} \\ 
        \dv{s} M_{X}(s) = \frac{\lambda}{(\lambda - s)^2}, &\quad \dv[2]{s} M_{X}(s) = \frac{2\lambda}{(\lambda - s)^3}
    \end{aligned}
    \end{equation}
    Thus, by setting $s = 0$, we have
    \begin{equation}
        \E[X] = \frac{1}{\lambda}, \quad \E[X^2] = \frac{2}{\lambda^2}
    \end{equation}
\end{example}


\section{Sum of independent RVs}
\begin{theorem}[Sum of Independent RVs]
    Let $X$ and $Y$ be independent RVs, and let $Z = X + Y$. The Transform associated with $Z$ is
    \begin{equation}
        M_{Z}(s) = M_{X}(s) M_{Y}(s)
    \end{equation}
\end{theorem}
\begin{proof}
    \begin{equation}
        M_{Z}(s) = \E[\e^{sZ}] = \E[\e^{s(X + Y)}] = \E[\e^{sX} \e^{sY}] = \E[\e^{sX}] \E[\e^{sY}] = M_{X}(s) M_{Y}(s)
    \end{equation}
    The 4th equality is because $X$ and $Y$ are independent.
\end{proof}
\begin{example}[Sum of Poisson RVs]
    Let $X_i \sim \text{Poi}(\lambda_i)$, $i = 1, 2, \cdots, n$ be independent Poisson RVs. Then the transform of $Z = \sum_{i=1}^{n} X_i$ is
    \begin{equation}
        M_{Z}(s) = \prod_{i=1}^{n} M_{X_i}(s) = \prod_{i=1}^{n} \exp\qty(\lambda_i(\e^{s} - 1)) = \exp\qty(\sum_{i=1}^{n} \lambda_i(\e^{s} - 1))
    \end{equation}
\end{example}
\begin{example}[Sum of Some RVs]
    \begin{itemize}
        \item Bernoulli RV: Let $X_1, X_2, \cdots, X_n$ be independent Bernoulli RVs with parameter $p$. The transform of a single Bernoulli RV is
        \begin{equation}
            M_{X_i}(s) = (1 - p)\e^{0s} + p\e^{1s} = 1 - p + p \e^{s}
        \end{equation}
        Therefore, the transform of $Z = \sum_{i=1}^{n} X_i$ is
        \begin{equation}
            M_{Z}(s) = \prod_{i=1}^{n} M_{X_i}(s) = \prod_{i=1}^{n} \qty(1 - p + p \e^{s}) = (1 - p + p \e^{s})^n
        \end{equation}
        which is a binomial distribution with parameters $n$ and $p$.
        \begin{proof}
            The transform of a binomial distributed RV $Z \sim \mathcal{B}(n, p)$ is
            \begin{equation}
                M_{Z}(s) = \sum_{k=0}^{n} \binom{n}{k} p^k (1 - p)^{n - k} \e^{ks} = \sum_{k=0}^{n} \binom{n}{k} \qty(p\e^s)^k (1 - p)^{n - k} = (1 - p + p \e^{s})^n
            \end{equation}
        \end{proof}
        \item Poisson RV: Let $X_1, X_2, \cdots, X_n$ be independent Poisson RVs with parameters $\lambda_1, \lambda_2, \cdots, \lambda_n$. The transform of $Z = \sum_{i=1}^{n} X_i$ is
        \begin{equation}
            \prod_{i=1}^{n} M_{X_i}(s) = \prod_{i=1}^{n} \e^{\lambda_i(\e^{s} - 1)} = \exp\qty(\sum_{i=1}^{n} \lambda_i(\e^{s} - 1)) \coloneqq \e^{\lambda (\e^{s} - 1)}
        \end{equation}
        which is also a Poisson RV with parameter $\lambda = \sum_{i=1}^{n} \lambda_i$.
        \item Normal RV: Let $X_1, X_2, \cdots, X_n$ be independent normal RVs with parameters $\mu_i$ and $\sigma_i^2$. The transform of $Z = \sum_{i=1}^{n} X_i$ is
        \begin{equation}
            M_{Z}(s) = \prod_{i=1}^{n} M_{X_i}(s) = \prod_{i=1}^{n} \e^{\sigma_i^2 s^2/2 + \mu_i s} = \exp \qty(\frac{s^2}{2} \sum_{i=1}^{n} \sigma_i^2 + s \sum_{i=1}^{n} \mu_i) \coloneqq \exp\qty(\frac{s^2}{2} \sigma^2 + s \mu)
        \end{equation}
        which is also a normal RV with parameters $\mu = \sum_{i=1}^{n} \mu_i$ and $\sigma^2 = \sum_{i=1}^{n} \sigma_i^2$.
    \end{itemize}
\end{example}


\section{Sum of A Random Number of Independent RVs}
\label{sec:sum-of-a-random-number-of-independent-RVs}
Consider the sum of a series of independent identically distributed (i.i.d.) RVs $Y = X_1 + X_2 + \cdots + X_N$, where $N$ is a random variable. 
\begin{itemize}
    \item Expectation of $Y$: \\ 
    Conditioning on $N$
    \begin{equation}
        \E[Y \mid N = n] = \E[X_1 + X_2 + \cdots + X_n] = n \E[X], \forall n \in \mathbb{N} \quad\Rightarrow\quad \E[Y \mid N] = N \E[X]
    \end{equation}
    Using the law of total expectation
    \begin{equation}
        \E[Y] = \E[\E[Y \mid N]] = \E[N] \E[X]
    \end{equation}
    \item Variance of $Y$: \\
    Conditioning on $N$
    \begin{align}
        &\begin{aligned}
            &\var(Y \mid N = n) = \var(X_1 + X_2 + \cdots + X_n) = n \cdot \var(X), \forall n \in \mathbb{N} \\ 
            &\Rightarrow\quad \var(Y \mid N) = N \var(X) 
        \end{aligned} \\
        &\E[\var(Y \mid N)] = \E[N] \var(X) \\ 
        &\var(\E[Y \mid N]) = \var(N \E[X]) = (\E[X])^2 \var(N) 
    \end{align}
    The third and fourth equation is because $\E[X], \var(X)$ is a constant in terms of $N$. So according to the law of total variance
    \begin{equation}
        \var(Y) = \E[\var(Y \mid N)] + \var(\E[Y \mid N]) = \E[N] \var(X) + (\E[X])^2 \var(N)
    \end{equation}
    \item Transform: \\ 
    Conditional on $N$
    \begin{equation}
        \E[\e^{sY} \mid N = n] = \E[\e^{s(X_1 + X_2 + \cdots + X_n)}] = \E[\e^{sX_1}]^n = \qty(M_{X}(s))^n
    \end{equation}
    The second equality is due to the independence of $X_i$'s. Now, using the law of total expectation
    \begin{equation}
        M_{Y}(s) = \E[\e^{sY}] = \E[\E[\e^{sY} \mid N]] = \sum_{n=0}^{+\infty} M_{X}(s)^n p_N(n) = \sum_{n=0}^{+\infty} \e^{n \ln M_{X}(s)} p_{N}(n)
    \end{equation}
    Comparing with the transform of $N$
    \begin{equation}
        M_{N}(s) = \sum_{n=0}^{+\infty} \e^{ns} p_{N}(n)
    \end{equation}
    We can see that the transform of $Y$ is the transform of $N$ with $s$ substituted by $\ln M_{X}(s)$.
 \end{itemize}
\begin{example}[Example of Sum of A Random Number of Independent RVs]
    Assume that $N$ and $X_i$ are both geometrically distributed with parameter $p, q$, respectively. All these RVs are independent and define $Y = X_1 + X_2 + \cdots + X_N$. \\ 
    We have
    \begin{equation}
        M_{N}(s) = \frac{p \e^s}{1 - (1-p) \e^s}, \quad M_{X}(s) = \frac{q \e^s}{1 - (1-q) \e^s}
    \end{equation}
    substitute $s$ with $\ln M_{X}(s)$ in $M_{N}(s)$
    \begin{equation}
        M_Y(s) = \frac{pq \e^s}{1 - (1-pq) \e^s}
    \end{equation}
\end{example}
