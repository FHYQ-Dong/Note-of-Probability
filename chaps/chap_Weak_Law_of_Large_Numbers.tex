\chapter{Weak Law of Large Numbers}

\section{Probabilistic Inequalities}
The intuition makes sense that if a non-negative RV must have a little probability to be large if its mean is small.
\begin{theorem}[Markov's Inequality]
    If a RV $X$ is non-negative, then for any $a > 0$,
    \begin{equation}
        \mathbf{P}(X \geq a) \leq \frac{\E[X]}{a}
    \end{equation}
\end{theorem}
\begin{proof}
    \begin{equation}
        \E[X] = \int_{0}^{+\infty} xf_X(x) \dd{x} \geq \int_{a}^{+\infty} xf_X(x) \dd{x} \geq a\int_{a}^{+\infty} f_X(x) \dd{x} = a \mathbf{P}(X \geq a)
    \end{equation}
\end{proof}
The equality holds iff. $\int_{0}^{a} xf_X(x) \dd{x} = 0$ and $\int_{a^+}^{+\infty} xf_X(x) \dd{x} = 0$, i.e. $X$ can only take the value $a$ and $0$.

The intuition makes sense that if a RV has small variance, then it is unlikely to be far away from its mean.
\begin{theorem}[Chebyshev's Inequality]
    If a RV $X$ has finite mean $\mu$ and finite variance $\sigma^2$, then for any $c > 0$,
    \begin{equation}
        \mathbf{P}(\abs{X - \mu} \geq c) \leq \frac{\sigma^2}{c^2}
    \end{equation}
\end{theorem}
\begin{proof}
    \begin{equation}
        \sigma^2 = \int_{-\infty}^{+\infty} (x - \mu)^2 f_X(x) \dd{x} \geq \int_{-\infty}^{\mu - c} (x - \mu)^2 f_X(x) \dd{x} + \int_{\mu + c}^{+\infty} (x - \mu)^2 f_X(x) \dd{x} \geq c^2 \mathbf{P}(\abs{X - \mu} \geq c)
    \end{equation}
\end{proof}
The equality holds iff. $\int_{(\mu-c)^+}^{(\mu+c)^-} (x - \mu)^2 f_X(x) \dd{x} = \int_{-\infty}^{(\mu-c)^-} (x - \mu)^2 f_X(x) \dd{x} = \int_{(\mu+c)^{+}}^{+\infty} (x - \mu)^2 f_X(x) \dd{x} = 0$, i.e. $X$ can only take the value $\mu - c$, $\mu + c$ and $\mu$, and $\mathbf{P}(X = \mu) = 1 - \sigma^2/c^2$. To keep the mean is still $\mu$, $\mathbf{P}(X = \mu + c) = \mathbf{P}(X = \mu - c) = \sigma^2/c^2$.

\begin{theorem}[Chernoff's Inequality]
    If a RV $X$ has a transform $M_{X}(s)$, then
    \begin{equation}
        \mathbf{P}(X \geq a) \leq \exp\qty(-\max_{s \geq 0} \qty{sa - \ln M_{X}(s)})
    \end{equation}
\end{theorem}
\begin{proof}
    Given $a$ and $\textcolor{red}{\forall s \geq 0}$, we have
    \begin{equation}
        M_X(s) = \int_{-\infty}^{a} \e^{sx} f_X(x) \dd{x} + \int_{a}^{+\infty} \e^{sx} f_X(x) \dd{x} \geq \int_{a}^{+\infty} \e^{sx} f_X(x) \dd{x} \geq \e^{sa} \mathbf{P}(X \geq a)
    \end{equation}
    which yields
    \begin{equation}
        \mathbf{P}(X \geq a) \leq \frac{M_X(s)}{\e^{sa}}, \forall s \geq 0 \quad\Rightarrow\quad \mathbf{P}(X \geq a) \leq \inf_{s \geq 0} \frac{M_X(s)}{\e^{sa}} = \exp\qty(-\max_{s \geq 0} \qty{sa - \ln M_{X}(s)})
    \end{equation}
\end{proof}
This inequality is useful when $a > 0$. Similarly, we can prove
\begin{theorem}[Chernoff's Inequality]
    If a RV $X$ has a transform $M_{X}(s)$, then
    \begin{equation}
        \mathbf{P}(X \leq a) \leq \exp\qty(-\max_{s \leq 0} \qty{sa - \ln M_{X}(s)})
    \end{equation}
\end{theorem}

The three inequalities have the upper bound of $c^{-1}$, $c^{-2}$ and $\exp(-c)$, respectively, with the information of the distribution of $X$ increasing. 

\begin{example}[Upper Bound of Variance]
    When $X$ is known to take values in a range $[a, b]$, then $\sigma^2 \leq (b - a)^2/4$.
\end{example}
\begin{proof}
    Note that for any constant $\gamma$
    \begin{equation}
        \E[(X - \gamma)^2] = \E[X^2] - 2\gamma \E[X] + \gamma^2
    \end{equation}
    which is minimized when $\gamma = \E[X]$. Therefore, letting $\gamma = (a+b)/2$
    \begin{equation}
        \sigma^2 = \E[X - \E[X]] \leq \E\qty[\qty(X - \frac{a+b}{2})^2] = \E\qty[(X - a)(X - b)] + \frac{(b - a)^2}{4} \leq \frac{(b - a)^2}{4}
    \end{equation}
\end{proof}


\section{Weak Law of Large Numbers}
When the number of samples becomes large, i.e. let $X_1, X_2, \cdots, X_n$ be i.i.d RVs with finite mean $\mu$ and finite variance $\sigma^2$, and let $n \to \infty$
\begin{equation}
    M_n = \frac{X_1 + X_2 + \cdots + X_n}{n}
\end{equation}
\begin{itemize}
    \item $\E[M_n] = \mu$
    \item $\var(M_n) = \frac{\sigma^2}{n}$
    \item Applying Chebyshev's inequality
    \begin{equation}
        \mathbf{P}(\abs{X - \mu} \geq \epsilon) \leq \frac{\sigma^2}{n\epsilon^2}
    \end{equation}
    \item Let $n \to \infty$, we have $\mathbf{P}(\abs{X - \mu} \geq \epsilon) \to 0$.
\end{itemize}
\begin{theorem}[The Weak Law of Large Numbers (Khinchin)]
    Let $X_1, X_2, \cdots, X_n$ be i.i.d RVs with finite mean $\mu$ and finite variance $\sigma^2$. Then for any $\epsilon > 0$,
    \begin{equation}
        \mathbf{P}(\abs{M_n - \mu} \geq \epsilon) = \mathbf{P}\qty(\abs{\frac{X_1 + X_2 + \cdots + X_n}{n} - \mu} \geq \epsilon) \leq \frac{\sigma^2}{n\epsilon^2} \to 0, \text{ as } n \to \infty
    \end{equation}
\end{theorem}
We say that $M_n$ converges \textcolor{red}{in probability} to $\mu$, denoted as $M_n \xrightarrow{\textcolor{red}{P}} \mu$.

\begin{example}[The pollster's problem]
    Let $f$ denote the fraction of voters who support a particular officer, and $X_i$ be Bernoulli random variables encoding poll responses
    \begin{equation}
        X_i = \left\{\begin{aligned}
            1, \quad &\text{if yes} \\
            0, \quad &\text{if no}
        \end{aligned}\right.
    \end{equation}
    The sample mean $M_n = (X_1 + \cdots + X_n) / n$ estimates the fraction of ``yes'' in our sample. The goal is to solve for $n$ satisfying 95\% confidence of $\leq 1\%$ error
    \begin{equation}
        \mathbf{P}(\abs{M_n - f} \geq 0.01) \leq 0.05
    \end{equation}
\end{example}
\begin{solution}
    Use the Chebyshev's inequality
    \begin{equation}
        \mathbf{P}(\abs{M_n - f} \geq 0.01) \leq \frac{\sigma_{M_n}^2}{(0.01)^2} = \frac{\sigma_X^2}{n(0.01)^2} \leq \frac{1}{4n(0.01)^2} \leq 0.05 \quad\Rightarrow\quad n \geq 50000
    \end{equation}
\end{solution}

We have three ways to estimates the sum $S_n$:
\begin{itemize}
    \item $S_n = X_1 + X_2 + \cdots$ with variance $n^2 \sigma^2$.
    \item $\frac{S_n}{n}$ with variance $\sigma^2/n$ (WLLN).
    \item $\frac{S_n}{\sqrt{n}}$ with variance $\sigma^2$ which is a constant (CLT).
\end{itemize}

\begin{theorem}[Convergence in Probability (To a Number)]
    Let ${Y_n}$ (or $Y_1 ,Y_2, \ldots$) be a sequence of RVs (not necessarily independent), and let $a$ be a real number. We say that the sequence $Y_n$ \textbf{converges to $a$ in probability}, if for every $\epsilon > 0$, we have
    \begin{equation}
        \lim_{n \to \infty} \mathbf{P}(\abs{Y_n - a} \geq \epsilon) = 0
    \end{equation}
\end{theorem}
``Convergence in probability'' means almost all PDF eventually gets concentrated around $a$.

\begin{remark}
    The value that $Y_n$ converges to can be different from the mean $\E[Y_n]$, e.g., when 
    \begin{equation}
        Y_n = \left\{\begin{aligned}
            &0, \quad &\text{w.p. } 1 - \frac{1}{n} \\
            &n, \quad &\text{w.p. } \frac{1}{n}
        \end{aligned}\right.
    \end{equation}
    which converges to $0$ in probability, but $\E[Y_n] = 1$, which is the cause of a \href{https://www.zhihu.com/question/570330301/answer/2789083568}{Zhihu question}.
\end{remark}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.3\textwidth]{images/Note-11.1.png}
    \caption{Convergence in Probability}
    \label{fig:convergence_in_probability}
\end{figure}
\begin{example}[Minimum of Multiple i.i.d. RVs]
    Consider a sequence of independent RVs $X_n$ that are uniformly distributed in the interval $[0, 1]$, and let
    \begin{equation}
        Y_n = \min(X_1, X_2, \cdots, X_n)
    \end{equation}
    Find the value which $Y_n$ converges to in probability.
\end{example}
\begin{solution}
    Because $X_i$ are i.i.d RV which are uniformly distributed in the interval $[0, 1]$
    \begin{equation}
        \mathbf{P}(\abs{Y_n - 0} \geq \epsilon) = \mathbf{P}(Y_n \geq \epsilon) = \mathbf{P}(X_1 \geq \epsilon, X_2 \geq \epsilon, \cdots, X_n \geq \epsilon) = (1 - \epsilon)^n \to 0, \text{ as } n \to \infty
    \end{equation}
\end{solution}


\section{Various Kinds of Convergence}
\begin{remark}
    \textbf{Here, both $X_n$ and $X$ are RVs on the same probability space $(\Omega, \mathcal{F}, \mathbf{P})$.}
\end{remark}
\begin{definition}[Convergence in Probability]
    We say that $X_n$ converges to $X$ in probability , written as $X_n \xrightarrow{P} X$, if
    \begin{equation}
        \lim_{n \to \infty} \mathbf{P}(\abs{X_n - X} \geq \epsilon) = 0, \quad \forall \epsilon > 0
    \end{equation}
\end{definition}
\begin{definition}[Convergence in Distribution]
    We say that $X_n$ converges to $X$ in distribution, written as $X_n \xrightarrow{D} X$, if
    \begin{equation}
        \lim_{n \to \infty} \mathbf{P}(X_n \leq x) = \mathbf{P}(X \leq x)
    \end{equation}
    for all $x$ where CDF $F_X(x) = \mathbf{P}(X \leq x)$ is continuous.
\end{definition}
\begin{definition}[Convergence Almost Surely]
    We say that $X_n$ converges to $X$ almost surely, written as $X_n \xrightarrow{a.s.} X$, if there is (a measurable) set $A \subset \Omega$ such that $\mathbf{P}(A) = 1$ and 
    \begin{equation}
        \mathbf{P}\qty(\qty{\omega: \lim_{n\to\infty}X_n(\omega) = X(\omega)}) = 1
    \end{equation}
\end{definition}
\begin{definition}[Convergence in Mean/in Norm]
    We say that $X_n$ converges to $X$ in $r$-th mean, written as $X_n \xrightarrow{r} X$, if
    \begin{equation}
        \lim_{n \to \infty} \E[\abs{X_n - X}^r] = 0
    \end{equation}
\end{definition}
\begin{remark}
    \begin{itemize}
        \item \textbf{In probability \& almost surely:} In convergence in probability we consider every value in $\mathbb{R}$, while in convergence almost surely we consider every element in $\Omega$. Due to the random variable is a $1$-to-$n$ mapping, convergence almost surely is more strong than convergence in probability.
        \item \textbf{In probability \& in distribution:} In convergence in distribution we only consider if the CDFs are same. But the two RVs may not be point-to-point convergent, e.g. $X = -Y$ and $f_Y(y) = f_Y(-y)$.
    \end{itemize}
\end{remark}

\begin{theorem}[Strength of Convergence]
    \begin{align}
        \qty(X_n \xrightarrow{a.s.} X) \Rightarrow \qty(X_n \xrightarrow{P} X) \Rightarrow \qty(X_n \xrightarrow{D} X) \\
        \qty(X_n \xrightarrow{r} X) \Rightarrow \qty(X_n \xrightarrow{P} X) \Rightarrow \qty(X_n \xrightarrow{D} X) \\
        \forall r \geq s \geq 1, \quad \qty(X_n \xrightarrow{r} X) \Rightarrow \qty(X_n \xrightarrow{s} X)
    \end{align}
\end{theorem}
