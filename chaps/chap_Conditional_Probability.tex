\chapter{Conditional Probability}
\section{Conditional Probability}
\begin{definition}[Conditional Probability]
    Conditional probability is the probability of an event $A$ given that another event $B$ has already occurred. It is denoted by $P(A|B)$ and is defined as:
    \begin{equation}
        \mathbf{P}(A|B) = \frac{\mathbf{P}(A \cap B)}{\mathbf{P}(B)}
    \end{equation}
    Note that when $\mathbf{P}(B) = 0$, the conditional probability is undefined.
\end{definition}

We can easily check the conditional probability satisfies the properties of probability measure, which means it is a legitimate probability on a new universe.
\begin{itemize}
    \item Non-negativity: $\mathbf{P}(A|B) \geq 0$.
    \item Normalization: $\mathbf{P}(\varOmega|B) = 1$.
    \item Additivity: when $A$ and $B$ are disjoint, $\mathbf{P}(A \cup B|C) = \frac{\mathbf{P}((A \cup B) \cap C)}{\mathbf{P}(C)} = \frac{\mathbf{P}(A \cap C) + \mathbf{P}(B \cap C)}{\mathbf{P}(C)} = \mathbf{P}(A|C) + \mathbf{P}(B|C)$. (The second equality is due to $A \cap C$ and $B \cap C$ are disjoint).
\end{itemize}

\begin{example}[Discrete and Continuous]
    For discrete case, the conditional probability can be calulated by:
    \begin{equation}
        \mathbf{P}(A|B) = \frac{\#~of~elements~in~A}{\#~of~elements~in~B}
    \end{equation}
    For continuous case, the conditional probability can be calculated by:
    \begin{equation}
        \mathbf{P}(A|B) = \frac{area~of~A}{area~of~B}
    \end{equation}
\end{example}

When solving a problem, the following equations may help:
\begin{align}
    \mathbf{P}(A \cap D) &= \mathbf{P}(A) \cdot \mathbf{P}(D|A) = \mathbf{P}(D) \cdot \mathbf{P}(A|D) \\
    \mathbf{P}(A \cap B \cap C) &= \mathbf{P}(A) \cdot \mathbf{P}(B|A) \cdot \mathbf{P}(C|A \cap B)
\end{align}
For the second equation: 1.event $A$ occurs; 2.event $B$ occurs given $A$; 3.event $C$ occurs given $A$ and $B$. Or
\begin{align*}
    \mathbf{P}(A \cap B \cap C) &= \mathbf{P}(A \cap B) \cdot \mathbf{P}(C | A \cap B) \\ 
    &= \mathbf{P}(A) \cdot \mathbf{P}(B | A) \cdot \mathbf{P}(C | A \cap B)
\end{align*}
\begin{remark}
    These equations are not fixed, depending on $\mathbf{P}(A)$, $\mathbf{P(A \cap B)}$ and $\mathbf{P}(A | B)$ which are easier to calculate.
\end{remark}

\section{Total Probability Theorem}
We can calculate the total probability with \textit{Devide and Conquer} strategy:
\begin{enumerate}
    \item Devide event $B$ by $A_1, A_2, \cdots A_n$, note that $\bigcup_{i=1}^n A_i = \varOmega$ and $A_i \cap A_j = \emptyset$.
    \item Calculate $\mathbf{P}(B | A_i)$ which are easier to calculate.
    \item Sum all the probabilities.
\end{enumerate}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.3\textwidth]{images/Note-2.1.png}
    \caption{Total Probability Theorem}
    \label{fig:total-probability}
\end{figure}

\begin{theorem}[Total Probability Theorem]
    Let $A_1, A_2, \cdots, A_n$ be a partition of the sample space $\varOmega$. Then for any event $B$,
    \begin{equation}
        \mathbf{P}(B) = \sum_{i=1}^{n} \mathbf{P}(A_i) \cdot \mathbf{P}(B | A_i)
    \end{equation}
\end{theorem}
\begin{example}[Die Rolling]
    You roll a fair four-sided die. If the result is 1 or 2, you roll once more but otherwise, you stop. What is the probability that the sum total of rolls is at least 4?
    \begin{solution}
        \begin{enumerate}
            \item Let $A_i = \{the~first~roll~is~i\}, B = \{the~sum~total~of~rolls~is~at~least~4\}$.
            \item $\mathbf{P}(A_1) = \mathbf{P}(A_2) = \mathbf{P}(A_3) = \mathbf{P}(A_4) = \frac{1}{4}$.
            \item $\mathbf{P}(B | A_1) = \frac{1}{2}, \mathbf{P}(B | A_2) = \frac{3}{4}, \mathbf{P}(B | A_3) = 0, \mathbf{P}(B | A_4) = 1$.
            \item $\mathbf{P}(B) = \frac{9}{16}$.
        \end{enumerate}
    \end{solution}
\end{example}


\section{Bayes' Theorem}
If we know:
\begin{itemize}
    \item ``Prior'' probabilities $\mathbf{P}(A_i)$.
    \item ``Likelihood'' probabilities $\mathbf{P}(B | A_i)$.
\end{itemize}
We wish we can calculate the ``Posterior'' probabilities $\mathbf{P}(A_i | B)$. 

\begin{theorem}[Bayes' Theorem]
    Let $A_1, A_2, \cdots, A_n$ be a partition of the sample space $\varOmega$. Then for any event $B$
    \begin{equation}
    \begin{aligned}
        \mathbf{P}(A_i | B) &= \frac{\mathbf{P}(A_i \cap B)}{\mathbf{P}(B)} \\
        &= \frac{\mathbf{P}(A_i) \cdot \mathbf{P}(B | A_i)}{\sum_{j=1}^{n} \mathbf{P}(A_j) \cdot \mathbf{P}(B | A_j)} \\
        &= \frac{\mathbf{P}(A_i) \cdot \mathbf{P}(B | A_i)}{\sum_{j=1}^{n} \mathbf{P}(A_j) \cdot \mathbf{P}(B | A_j)}
    \end{aligned}
    \end{equation}
\end{theorem}

A: cause, B: result. Bayes' Theorem is used to calculate the cause given the result. \textit{(inference based on probability)}
